{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30b81c0",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "This project aims to train a perceptron model to behave as a simple logic gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad76b16",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "\n",
    "\n",
    "2. [Logic Gates](#logic)\n",
    "\n",
    "\n",
    "3. [The Perceptron Algorithm](#tpa)\n",
    "    1. [Perceptron Struct](#struct)\n",
    "    2. [Forward Pass](#forward)\n",
    "    3. [Activation Function](#af)\n",
    "    \n",
    "    \n",
    "4. [The Perceptron Training Rule](#tptr)\n",
    "    1. [Error Function](#error)\n",
    "    2. [Delta W](#deltaw)\n",
    "    3. [Update Weights](#uweights)\n",
    "    4. [Backward Pass](#bpass)\n",
    "    \n",
    "    \n",
    "5. [Training](#train)\n",
    "     1. [Import Data](#imdata)\n",
    "     2. [AND-perceptron](#tandp)\n",
    "     3. [OR-perceptron](#torp)\n",
    "     \n",
    "     \n",
    "6. [Results](#results)\n",
    "    1. [Output](#output)\n",
    "    2. [AND-perceptron](#apr)\n",
    "    3. [OR-perceptron](#opr)\n",
    "\n",
    "\n",
    "7. [Conclusion](#conc)\n",
    "\n",
    "\n",
    "8. [References](#ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05cff6",
   "metadata": {},
   "source": [
    "# Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "The perceptron is a supervised machine learning model for regression and classification problems. The algorithm was developed in the late 1950's by neurobiology professor Frank Rosenblatt of Cornell University. The perceptron is often referred to as an artificial neuron due to it functioning similarly to natural neurons from the brain. Greatly inspired by the way natural neurons function, Rosenblatt worked with the following question in mind:\n",
    "\n",
    "\"What are the minimum things that a brain has to have physicially in order to perform the amazing things it does?\"\n",
    "\n",
    "Today perceptrons are the building blocks of all modern deep learning architectures, they can be considered to be a single node/cell/neuron that's able to classify linearly seperable input data. When multiple neurons are connected in sequence, the resulting neural network is able to perform more complex computations. Complex neural networks can be observed in both deep artificial neural nets as well as in the brain.\n",
    "\n",
    "The perceptron in this project will be trained to behave as an AND/OR logic gate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be949c",
   "metadata": {},
   "source": [
    "# Logic Gates <a class=\"anchor\" id=\"logic\"></a>\n",
    "\n",
    "Logic gates are boolean functions that take in one or more inputs and produce an output of either true or false. Specific logics determine the relationship between the input and output. Perceptrons are able to learn these logics through training and perform as logic gates.\n",
    "\n",
    "Here are a few examples of logic gates:\n",
    "\n",
    "#### AND Gate\n",
    "An AND gate takes in two inputs A and B. The output of the AND gate is true, if and only if inputs A and B are both true. Otherwise the output of the AND gate is false.\n",
    "\n",
    "#### OR Gate\n",
    "An OR gate takes in two inputs A and B. The output of the OR Gate is true if either of A and B are true, meaning the OR gate is false if and only if inputs A and B are both false.\n",
    "\n",
    "#### NAND Gate\n",
    "A NAND gate (or NOT-AND gate) takes in two inputs A and B and gives the opposite output that of which an AND gate would produce. A NAND gate is false if and only if inputs A and B are both true. \n",
    "\n",
    "#### Computational Universality\n",
    "NAND gates - as well as NOR gates - have a special trait in that they are universl for computation. This means for any given digital logic circuit, regardless of it's complexity, said circuit can be implemented as a network of NAND gates. Perceptrons are universal for computation via simulation of these gates.\n",
    "\n",
    "#### AND/OR/NAND gate Diagram\n",
    "![alt text](logic_gate_diagram.png \"Logic Gates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc1351",
   "metadata": {},
   "source": [
    "# The Perceptron Algorithm <a class=\"anchor\" id=\"tpa\"></a>\n",
    "\n",
    "Like a logic gate, a perceptron can recieve one or more input signals and produce an output. Perceptrons have a weight value for each input value. To produce an output, the sum of the inputs and weights are calculated and sent through to a non-linear activation function. The weights can be trained using 'The Perceptron Training Rule'.\n",
    "\n",
    "#### Perceptron Diagram\n",
    "![alt text](perceptron_diagram.png \"Perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b5190",
   "metadata": {},
   "source": [
    "#### Perceptron Struct <a class=\"anchor\" id=\"struct\"></a>\n",
    "\n",
    "Our perceptron will be built as a Julia struct containing four values: an input vector $X$, a weights vector $W$, a bias integer **$b$** and an ouptut integer **$ŷ$**. \n",
    "\n",
    "$W$ will be initialised with random values to be refined during training. \n",
    "\n",
    "$n$ describes the number of inputs, $X$ will have an initial length of $n$ and $W$ will have a length of $n+1$. \n",
    "\n",
    "The perceptron will have a bias of $1$ and an initial output of $0$.\n",
    "\n",
    "\\begin{align}\n",
    "Input = X_{n} &= \\begin{bmatrix}\n",
    " x_{1} \\\\ x_{2} \\\\ . \\\\ . \\\\ x_{n}\n",
    "\\end{bmatrix}\n",
    ",\\hspace{1cm} Weights = W_{n} = \\begin{bmatrix}\n",
    " w_{0} \\\\ w_{1} \\\\ . \\\\ . \\\\ w_{n}\n",
    "\\end{bmatrix}\n",
    ",\\hspace{1cm}Bias = b = 1\n",
    ",\\hspace{1cm}Initial \\hspace{0.1cm} Output = ŷ = 0.  \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7797e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= \n",
    "\n",
    "Perceptron struct\n",
    "    \n",
    "    Contains:\n",
    "        - X: n-element Input Vector.\n",
    "        - W: (n+1)-element Weights Vector.\n",
    "        - b: Bias Integer.\n",
    "        - ŷ: Output Integer.\n",
    "\n",
    "    Note:\n",
    "        Perceptron struct must be Mutable to allow for variables to be changed after decleration.\n",
    "=#\n",
    "\n",
    "mutable struct Perceptron\n",
    "    X::Vector{Int64}\n",
    "    W::Vector{Float16}\n",
    "    b::Int64\n",
    "    ŷ::Int64\n",
    "    \n",
    "    function Perceptron(n)\n",
    "        new(\n",
    "            zeros(n), \n",
    "            rand(Float16, n + 1), \n",
    "            1, \n",
    "            0\n",
    "        )\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304b23f",
   "metadata": {},
   "source": [
    "#### Forward Pass <a class=\"anchor\" id=\"forward\"></a>\n",
    "\n",
    "Part of the job of a perceptron is to calculate the weighted sum of it's inputs and produce a pre-activation value $z$. The weighted sum of a perceptron can be modeled as the dot product of $W$ and $X$ add the bias term **$b$**.\n",
    "\n",
    "**$z$** is then fed into an activation function $Θ$ which will either fire (return $1$) or not fire (return $0$), depending on the characteristics of **$z$**. \n",
    "\n",
    "There are many activation functions to choose from, each with distinct characteristics. We'll be using a threshold function described in the next section.\n",
    "\n",
    "Here's how a perceptron produces an output:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\\\\n",
    "\\hspace{0.3cm} \\Large z =  \\Sigma (weights * inputs) + bias\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\Large  ŷ = \\theta(z)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efd49a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward_pass (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "#= \n",
    "Forward pass through the perceptron\n",
    "    \n",
    "    1. Append the bias integer (b) to the input vector (X).\n",
    "    2. Take the weighted sum/dot product (z) of the input vector (inputs) and weights vector (weights).\n",
    "    3. Assign the Heaviside step function of the weighted sum to the Perceptrons' output (ŷ).\n",
    "\n",
    "    Parameters:\n",
    "        Some perceptron 'p'.\n",
    "\n",
    "    Returns:\n",
    "        Some integer output 'ŷ'.\n",
    "=#\n",
    "\n",
    "function forward_pass(p::Perceptron)\n",
    "    \n",
    "        push!(p.X, p.b)\n",
    "    \n",
    "        z = dot(p.X, p.W)\n",
    "    \n",
    "        p.ŷ = H(z)\n",
    "    \n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fcfd9",
   "metadata": {},
   "source": [
    "#### Activation Function  (Heaviside Step Function) <a class=\"anchor\" id=\"af\"></a>\n",
    "\n",
    "Activation functions dictate the output of a perceptron.\n",
    "\n",
    "A threshold activation function takes in some input $z$ and evaluates it against some threshold value $t$. \n",
    "\n",
    "The threshold function only fires when $z > t$ (where the input is above the threshold value). Otherwise if $z ≤ t$ the function does not fire.\n",
    "\n",
    "The Heaviside Step Function is a piecewise function frequently used in Fourier transforms. The function takes some input $z$ and returns $1$ if $z > 0$; otherwise it'll return $0$.\n",
    "\n",
    "The Heaviside Step Function:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Large \n",
    " H(z) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 & \\quad z \\leq 0 \\\\\n",
    "            1 & \\quad z > 0\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbaa769b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip750\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip750)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip751\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip750)\" d=\"\n",
       "M249.542 1423.18 L2352.76 1423.18 L2352.76 123.472 L249.542 123.472  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip752\">\n",
       "    <rect x=\"249\" y=\"123\" width=\"2104\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  507.483,1423.18 507.483,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  904.316,1423.18 904.316,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1301.15,1423.18 1301.15,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1697.98,1423.18 1697.98,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2094.81,1423.18 2094.81,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  507.483,1423.18 507.483,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  904.316,1423.18 904.316,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1301.15,1423.18 1301.15,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1697.98,1423.18 1697.98,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2094.81,1423.18 2094.81,1404.28 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip750)\" d=\"M476.384 1468.75 L506.06 1468.75 L506.06 1472.69 L476.384 1472.69 L476.384 1468.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M528.999 1455.09 L517.194 1473.54 L528.999 1473.54 L528.999 1455.09 M527.773 1451.02 L533.652 1451.02 L533.652 1473.54 L538.583 1473.54 L538.583 1477.43 L533.652 1477.43 L533.652 1485.58 L528.999 1485.58 L528.999 1477.43 L513.398 1477.43 L513.398 1472.92 L527.773 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M874.258 1468.75 L903.934 1468.75 L903.934 1472.69 L874.258 1472.69 L874.258 1468.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M918.055 1481.64 L934.374 1481.64 L934.374 1485.58 L912.43 1485.58 L912.43 1481.64 Q915.092 1478.89 919.675 1474.26 Q924.281 1469.61 925.462 1468.27 Q927.707 1465.74 928.587 1464.01 Q929.49 1462.25 929.49 1460.56 Q929.49 1457.8 927.545 1456.07 Q925.624 1454.33 922.522 1454.33 Q920.323 1454.33 917.869 1455.09 Q915.439 1455.86 912.661 1457.41 L912.661 1452.69 Q915.485 1451.55 917.939 1450.97 Q920.392 1450.39 922.429 1450.39 Q927.8 1450.39 930.994 1453.08 Q934.189 1455.77 934.189 1460.26 Q934.189 1462.39 933.379 1464.31 Q932.591 1466.2 930.485 1468.8 Q929.906 1469.47 926.804 1472.69 Q923.703 1475.88 918.055 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1301.15 1454.1 Q1297.54 1454.1 1295.71 1457.66 Q1293.9 1461.2 1293.9 1468.33 Q1293.9 1475.44 1295.71 1479.01 Q1297.54 1482.55 1301.15 1482.55 Q1304.78 1482.55 1306.59 1479.01 Q1308.42 1475.44 1308.42 1468.33 Q1308.42 1461.2 1306.59 1457.66 Q1304.78 1454.1 1301.15 1454.1 M1301.15 1450.39 Q1306.96 1450.39 1310.01 1455 Q1313.09 1459.58 1313.09 1468.33 Q1313.09 1477.06 1310.01 1481.67 Q1306.96 1486.25 1301.15 1486.25 Q1295.34 1486.25 1292.26 1481.67 Q1289.2 1477.06 1289.2 1468.33 Q1289.2 1459.58 1292.26 1455 Q1295.34 1450.39 1301.15 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1692.63 1481.64 L1708.95 1481.64 L1708.95 1485.58 L1687.01 1485.58 L1687.01 1481.64 Q1689.67 1478.89 1694.25 1474.26 Q1698.86 1469.61 1700.04 1468.27 Q1702.29 1465.74 1703.17 1464.01 Q1704.07 1462.25 1704.07 1460.56 Q1704.07 1457.8 1702.13 1456.07 Q1700.2 1454.33 1697.1 1454.33 Q1694.9 1454.33 1692.45 1455.09 Q1690.02 1455.86 1687.24 1457.41 L1687.24 1452.69 Q1690.07 1451.55 1692.52 1450.97 Q1694.97 1450.39 1697.01 1450.39 Q1702.38 1450.39 1705.57 1453.08 Q1708.77 1455.77 1708.77 1460.26 Q1708.77 1462.39 1707.96 1464.31 Q1707.17 1466.2 1705.07 1468.8 Q1704.49 1469.47 1701.38 1472.69 Q1698.28 1475.88 1692.63 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M2097.82 1455.09 L2086.02 1473.54 L2097.82 1473.54 L2097.82 1455.09 M2096.6 1451.02 L2102.48 1451.02 L2102.48 1473.54 L2107.41 1473.54 L2107.41 1477.43 L2102.48 1477.43 L2102.48 1485.58 L2097.82 1485.58 L2097.82 1477.43 L2082.22 1477.43 L2082.22 1472.92 L2096.6 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1287.64 1532.4 L1315.46 1532.4 L1315.46 1537.74 L1293.43 1563.37 L1315.46 1563.37 L1315.46 1568.04 L1286.84 1568.04 L1286.84 1562.7 L1308.87 1537.07 L1287.64 1537.07 L1287.64 1532.4 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,1386.4 2352.76,1386.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,1079.86 2352.76,1079.86 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,773.326 2352.76,773.326 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,466.791 2352.76,466.791 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip752)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,160.256 2352.76,160.256 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,1423.18 249.542,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,1386.4 268.44,1386.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,1079.86 268.44,1079.86 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,773.326 268.44,773.326 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,466.791 268.44,466.791 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip750)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,160.256 268.44,160.256 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip750)\" d=\"M126.205 1372.19 Q122.593 1372.19 120.765 1375.76 Q118.959 1379.3 118.959 1386.43 Q118.959 1393.54 120.765 1397.1 Q122.593 1400.64 126.205 1400.64 Q129.839 1400.64 131.644 1397.1 Q133.473 1393.54 133.473 1386.43 Q133.473 1379.3 131.644 1375.76 Q129.839 1372.19 126.205 1372.19 M126.205 1368.49 Q132.015 1368.49 135.07 1373.1 Q138.149 1377.68 138.149 1386.43 Q138.149 1395.16 135.07 1399.76 Q132.015 1404.35 126.205 1404.35 Q120.394 1404.35 117.316 1399.76 Q114.26 1395.16 114.26 1386.43 Q114.26 1377.68 117.316 1373.1 Q120.394 1368.49 126.205 1368.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M146.366 1397.8 L151.251 1397.8 L151.251 1403.68 L146.366 1403.68 L146.366 1397.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M171.436 1372.19 Q167.825 1372.19 165.996 1375.76 Q164.19 1379.3 164.19 1386.43 Q164.19 1393.54 165.996 1397.1 Q167.825 1400.64 171.436 1400.64 Q175.07 1400.64 176.876 1397.1 Q178.704 1393.54 178.704 1386.43 Q178.704 1379.3 176.876 1375.76 Q175.07 1372.19 171.436 1372.19 M171.436 1368.49 Q177.246 1368.49 180.301 1373.1 Q183.38 1377.68 183.38 1386.43 Q183.38 1395.16 180.301 1399.76 Q177.246 1404.35 171.436 1404.35 Q165.626 1404.35 162.547 1399.76 Q159.491 1395.16 159.491 1386.43 Q159.491 1377.68 162.547 1373.1 Q165.626 1368.49 171.436 1368.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M201.598 1372.19 Q197.987 1372.19 196.158 1375.76 Q194.352 1379.3 194.352 1386.43 Q194.352 1393.54 196.158 1397.1 Q197.987 1400.64 201.598 1400.64 Q205.232 1400.64 207.037 1397.1 Q208.866 1393.54 208.866 1386.43 Q208.866 1379.3 207.037 1375.76 Q205.232 1372.19 201.598 1372.19 M201.598 1368.49 Q207.408 1368.49 210.463 1373.1 Q213.542 1377.68 213.542 1386.43 Q213.542 1395.16 210.463 1399.76 Q207.408 1404.35 201.598 1404.35 Q195.787 1404.35 192.709 1399.76 Q189.653 1395.16 189.653 1386.43 Q189.653 1377.68 192.709 1373.1 Q195.787 1368.49 201.598 1368.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M127.2 1065.66 Q123.589 1065.66 121.76 1069.22 Q119.955 1072.77 119.955 1079.9 Q119.955 1087 121.76 1090.57 Q123.589 1094.11 127.2 1094.11 Q130.834 1094.11 132.64 1090.57 Q134.468 1087 134.468 1079.9 Q134.468 1072.77 132.64 1069.22 Q130.834 1065.66 127.2 1065.66 M127.2 1061.96 Q133.01 1061.96 136.066 1066.56 Q139.144 1071.15 139.144 1079.9 Q139.144 1088.62 136.066 1093.23 Q133.01 1097.81 127.2 1097.81 Q121.39 1097.81 118.311 1093.23 Q115.256 1088.62 115.256 1079.9 Q115.256 1071.15 118.311 1066.56 Q121.39 1061.96 127.2 1061.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M147.362 1091.26 L152.246 1091.26 L152.246 1097.14 L147.362 1097.14 L147.362 1091.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M166.459 1093.21 L182.778 1093.21 L182.778 1097.14 L160.834 1097.14 L160.834 1093.21 Q163.496 1090.45 168.079 1085.82 Q172.686 1081.17 173.866 1079.83 Q176.112 1077.3 176.991 1075.57 Q177.894 1073.81 177.894 1072.12 Q177.894 1069.36 175.95 1067.63 Q174.028 1065.89 170.927 1065.89 Q168.727 1065.89 166.274 1066.65 Q163.843 1067.42 161.065 1068.97 L161.065 1064.25 Q163.89 1063.11 166.343 1062.53 Q168.797 1061.96 170.834 1061.96 Q176.204 1061.96 179.399 1064.64 Q182.593 1067.33 182.593 1071.82 Q182.593 1073.95 181.783 1075.87 Q180.996 1077.77 178.889 1080.36 Q178.311 1081.03 175.209 1084.25 Q172.107 1087.44 166.459 1093.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M192.639 1062.58 L210.996 1062.58 L210.996 1066.52 L196.922 1066.52 L196.922 1074.99 Q197.94 1074.64 198.959 1074.48 Q199.977 1074.29 200.996 1074.29 Q206.783 1074.29 210.162 1077.47 Q213.542 1080.64 213.542 1086.05 Q213.542 1091.63 210.07 1094.73 Q206.598 1097.81 200.278 1097.81 Q198.102 1097.81 195.834 1097.44 Q193.588 1097.07 191.181 1096.33 L191.181 1091.63 Q193.264 1092.77 195.487 1093.32 Q197.709 1093.88 200.186 1093.88 Q204.19 1093.88 206.528 1091.77 Q208.866 1089.66 208.866 1086.05 Q208.866 1082.44 206.528 1080.34 Q204.19 1078.23 200.186 1078.23 Q198.311 1078.23 196.436 1078.65 Q194.584 1079.06 192.639 1079.94 L192.639 1062.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M126.205 759.125 Q122.593 759.125 120.765 762.689 Q118.959 766.231 118.959 773.361 Q118.959 780.467 120.765 784.032 Q122.593 787.574 126.205 787.574 Q129.839 787.574 131.644 784.032 Q133.473 780.467 133.473 773.361 Q133.473 766.231 131.644 762.689 Q129.839 759.125 126.205 759.125 M126.205 755.421 Q132.015 755.421 135.07 760.027 Q138.149 764.611 138.149 773.361 Q138.149 782.088 135.07 786.694 Q132.015 791.277 126.205 791.277 Q120.394 791.277 117.316 786.694 Q114.26 782.088 114.26 773.361 Q114.26 764.611 117.316 760.027 Q120.394 755.421 126.205 755.421 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M146.366 784.726 L151.251 784.726 L151.251 790.606 L146.366 790.606 L146.366 784.726 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M161.482 756.046 L179.839 756.046 L179.839 759.981 L165.765 759.981 L165.765 768.453 Q166.783 768.106 167.802 767.944 Q168.82 767.759 169.839 767.759 Q175.626 767.759 179.005 770.93 Q182.385 774.101 182.385 779.518 Q182.385 785.097 178.913 788.199 Q175.44 791.277 169.121 791.277 Q166.945 791.277 164.677 790.907 Q162.431 790.537 160.024 789.796 L160.024 785.097 Q162.107 786.231 164.329 786.787 Q166.552 787.342 169.028 787.342 Q173.033 787.342 175.371 785.236 Q177.709 783.129 177.709 779.518 Q177.709 775.907 175.371 773.801 Q173.033 771.694 169.028 771.694 Q167.153 771.694 165.278 772.111 Q163.427 772.527 161.482 773.407 L161.482 756.046 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M201.598 759.125 Q197.987 759.125 196.158 762.689 Q194.352 766.231 194.352 773.361 Q194.352 780.467 196.158 784.032 Q197.987 787.574 201.598 787.574 Q205.232 787.574 207.037 784.032 Q208.866 780.467 208.866 773.361 Q208.866 766.231 207.037 762.689 Q205.232 759.125 201.598 759.125 M201.598 755.421 Q207.408 755.421 210.463 760.027 Q213.542 764.611 213.542 773.361 Q213.542 782.088 210.463 786.694 Q207.408 791.277 201.598 791.277 Q195.787 791.277 192.709 786.694 Q189.653 782.088 189.653 773.361 Q189.653 764.611 192.709 760.027 Q195.787 755.421 201.598 755.421 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M127.2 452.59 Q123.589 452.59 121.76 456.155 Q119.955 459.696 119.955 466.826 Q119.955 473.932 121.76 477.497 Q123.589 481.039 127.2 481.039 Q130.834 481.039 132.64 477.497 Q134.468 473.932 134.468 466.826 Q134.468 459.696 132.64 456.155 Q130.834 452.59 127.2 452.59 M127.2 448.886 Q133.01 448.886 136.066 453.493 Q139.144 458.076 139.144 466.826 Q139.144 475.553 136.066 480.159 Q133.01 484.742 127.2 484.742 Q121.39 484.742 118.311 480.159 Q115.256 475.553 115.256 466.826 Q115.256 458.076 118.311 453.493 Q121.39 448.886 127.2 448.886 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M147.362 478.192 L152.246 478.192 L152.246 484.071 L147.362 484.071 L147.362 478.192 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M161.251 449.511 L183.473 449.511 L183.473 451.502 L170.927 484.071 L166.042 484.071 L177.848 453.446 L161.251 453.446 L161.251 449.511 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M192.639 449.511 L210.996 449.511 L210.996 453.446 L196.922 453.446 L196.922 461.918 Q197.94 461.571 198.959 461.409 Q199.977 461.224 200.996 461.224 Q206.783 461.224 210.162 464.395 Q213.542 467.567 213.542 472.983 Q213.542 478.562 210.07 481.664 Q206.598 484.742 200.278 484.742 Q198.102 484.742 195.834 484.372 Q193.588 484.002 191.181 483.261 L191.181 478.562 Q193.264 479.696 195.487 480.252 Q197.709 480.807 200.186 480.807 Q204.19 480.807 206.528 478.701 Q208.866 476.594 208.866 472.983 Q208.866 469.372 206.528 467.266 Q204.19 465.159 200.186 465.159 Q198.311 465.159 196.436 465.576 Q194.584 465.993 192.639 466.872 L192.639 449.511 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M117.015 173.601 L124.654 173.601 L124.654 147.236 L116.343 148.902 L116.343 144.643 L124.607 142.976 L129.283 142.976 L129.283 173.601 L136.922 173.601 L136.922 177.536 L117.015 177.536 L117.015 173.601 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M146.366 171.657 L151.251 171.657 L151.251 177.536 L146.366 177.536 L146.366 171.657 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M171.436 146.055 Q167.825 146.055 165.996 149.62 Q164.19 153.161 164.19 160.291 Q164.19 167.397 165.996 170.962 Q167.825 174.504 171.436 174.504 Q175.07 174.504 176.876 170.962 Q178.704 167.397 178.704 160.291 Q178.704 153.161 176.876 149.62 Q175.07 146.055 171.436 146.055 M171.436 142.351 Q177.246 142.351 180.301 146.958 Q183.38 151.541 183.38 160.291 Q183.38 169.018 180.301 173.624 Q177.246 178.208 171.436 178.208 Q165.626 178.208 162.547 173.624 Q159.491 169.018 159.491 160.291 Q159.491 151.541 162.547 146.958 Q165.626 142.351 171.436 142.351 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M201.598 146.055 Q197.987 146.055 196.158 149.62 Q194.352 153.161 194.352 160.291 Q194.352 167.397 196.158 170.962 Q197.987 174.504 201.598 174.504 Q205.232 174.504 207.037 170.962 Q208.866 167.397 208.866 160.291 Q208.866 153.161 207.037 149.62 Q205.232 146.055 201.598 146.055 M201.598 142.351 Q207.408 142.351 210.463 146.958 Q213.542 151.541 213.542 160.291 Q213.542 169.018 210.463 173.624 Q207.408 178.208 201.598 178.208 Q195.787 178.208 192.709 173.624 Q189.653 169.018 189.653 160.291 Q189.653 151.541 192.709 146.958 Q195.787 142.351 201.598 142.351 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M16.4842 834.373 L16.4842 827.944 L35.9632 827.944 L35.9632 804.582 L16.4842 804.582 L16.4842 798.152 L64.0042 798.152 L64.0042 804.582 L41.3741 804.582 L41.3741 827.944 L64.0042 827.944 L64.0042 834.373 L16.4842 834.373 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M14.5426 771.544 Q21.8632 775.809 29.0246 777.877 Q36.186 779.946 43.5384 779.946 Q50.8908 779.946 58.1159 777.877 Q65.3091 775.777 72.5979 771.544 L72.5979 776.636 Q65.1182 781.41 57.8931 783.798 Q50.668 786.153 43.5384 786.153 Q36.4406 786.153 29.2474 783.798 Q22.0542 781.442 14.5426 776.636 L14.5426 771.544 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M28.3562 762.727 L28.3562 734.909 L33.7034 734.909 L59.3254 756.934 L59.3254 734.909 L64.0042 734.909 L64.0042 763.523 L58.657 763.523 L33.035 741.497 L33.035 762.727 L28.3562 762.727 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M14.5426 726.888 L14.5426 721.796 Q22.0542 717.021 29.2474 714.666 Q36.4406 712.279 43.5384 712.279 Q50.668 712.279 57.8931 714.666 Q65.1182 717.021 72.5979 721.796 L72.5979 726.888 Q65.3091 722.655 58.1159 720.586 Q50.8908 718.485 43.5384 718.485 Q36.186 718.485 29.0246 720.586 Q21.8632 722.655 14.5426 726.888 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M808.296 12.096 L816.479 12.096 L816.479 36.8875 L846.212 36.8875 L846.212 12.096 L854.395 12.096 L854.395 72.576 L846.212 72.576 L846.212 43.7741 L816.479 43.7741 L816.479 72.576 L808.296 72.576 L808.296 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M909.163 48.0275 L909.163 51.6733 L874.893 51.6733 Q875.379 59.3701 879.511 63.421 Q883.683 67.4314 891.096 67.4314 Q895.39 67.4314 899.401 66.3781 Q903.451 65.3249 907.421 63.2184 L907.421 70.267 Q903.411 71.9684 899.198 72.8596 Q894.985 73.7508 890.651 73.7508 Q879.794 73.7508 873.434 67.4314 Q867.115 61.1119 867.115 50.3365 Q867.115 39.1965 873.11 32.6746 Q879.146 26.1121 889.354 26.1121 Q898.509 26.1121 903.816 32.0264 Q909.163 37.9003 909.163 48.0275 M901.71 45.84 Q901.629 39.7232 898.266 36.0774 Q894.945 32.4315 889.435 32.4315 Q883.197 32.4315 879.43 35.9558 Q875.703 39.4801 875.136 45.8805 L901.71 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M942.016 49.7694 Q932.983 49.7694 929.499 51.8354 Q926.015 53.9013 926.015 58.8839 Q926.015 62.8538 928.608 65.2034 Q931.241 67.5124 935.737 67.5124 Q941.935 67.5124 945.662 63.1374 Q949.429 58.7219 949.429 51.4303 L949.429 49.7694 L942.016 49.7694 M956.883 46.6907 L956.883 72.576 L949.429 72.576 L949.429 65.6895 Q946.877 69.8214 943.069 71.8063 Q939.261 73.7508 933.752 73.7508 Q926.785 73.7508 922.653 69.8619 Q918.561 65.9325 918.561 59.3701 Q918.561 51.7138 923.665 47.825 Q928.81 43.9361 938.978 43.9361 L949.429 43.9361 L949.429 43.2069 Q949.429 38.0623 946.026 35.2672 Q942.664 32.4315 936.547 32.4315 Q932.658 32.4315 928.972 33.3632 Q925.286 34.295 921.883 36.1584 L921.883 29.2718 Q925.974 27.692 929.823 26.9223 Q933.671 26.1121 937.317 26.1121 Q947.161 26.1121 952.022 31.2163 Q956.883 36.3204 956.883 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M966.889 27.2059 L974.788 27.2059 L988.966 65.2844 L1003.14 27.2059 L1011.04 27.2059 L994.03 72.576 L983.902 72.576 L966.889 27.2059 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1021.33 27.2059 L1028.79 27.2059 L1028.79 72.576 L1021.33 72.576 L1021.33 27.2059 M1021.33 9.54393 L1028.79 9.54393 L1028.79 18.9825 L1021.33 18.9825 L1021.33 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1073.31 28.5427 L1073.31 35.5912 Q1070.15 33.9709 1066.74 33.1607 Q1063.34 32.3505 1059.69 32.3505 Q1054.15 32.3505 1051.35 34.0519 Q1048.6 35.7533 1048.6 39.156 Q1048.6 41.7486 1050.58 43.2475 Q1052.57 44.7058 1058.56 46.0426 L1061.11 46.6097 Q1069.05 48.3111 1072.37 51.4303 Q1075.74 54.509 1075.74 60.0587 Q1075.74 66.3781 1070.71 70.0644 Q1065.73 73.7508 1056.98 73.7508 Q1053.33 73.7508 1049.37 73.0216 Q1045.44 72.3329 1041.06 70.9151 L1041.06 63.2184 Q1045.19 65.3654 1049.2 66.4591 Q1053.21 67.5124 1057.14 67.5124 Q1062.41 67.5124 1065.24 65.73 Q1068.08 63.9071 1068.08 60.6258 Q1068.08 57.5877 1066.01 55.9673 Q1063.99 54.3469 1057.06 52.8481 L1054.47 52.2405 Q1047.54 50.7821 1044.46 47.7845 Q1041.38 44.7463 1041.38 39.4801 Q1041.38 33.0797 1045.92 29.5959 Q1050.46 26.1121 1058.8 26.1121 Q1062.94 26.1121 1066.58 26.7198 Q1070.23 27.3274 1073.31 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1087.61 27.2059 L1095.06 27.2059 L1095.06 72.576 L1087.61 72.576 L1087.61 27.2059 M1087.61 9.54393 L1095.06 9.54393 L1095.06 18.9825 L1087.61 18.9825 L1087.61 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1140.51 34.0924 L1140.51 9.54393 L1147.96 9.54393 L1147.96 72.576 L1140.51 72.576 L1140.51 65.7705 Q1138.16 69.8214 1134.56 71.8063 Q1130.99 73.7508 1125.97 73.7508 Q1117.74 73.7508 1112.56 67.1883 Q1107.41 60.6258 1107.41 49.9314 Q1107.41 39.2371 1112.56 32.6746 Q1117.74 26.1121 1125.97 26.1121 Q1130.99 26.1121 1134.56 28.0971 Q1138.16 30.0415 1140.51 34.0924 M1115.11 49.9314 Q1115.11 58.1548 1118.47 62.8538 Q1121.88 67.5124 1127.79 67.5124 Q1133.7 67.5124 1137.11 62.8538 Q1140.51 58.1548 1140.51 49.9314 Q1140.51 41.7081 1137.11 37.0496 Q1133.7 32.3505 1127.79 32.3505 Q1121.88 32.3505 1118.47 37.0496 Q1115.11 41.7081 1115.11 49.9314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1202.12 48.0275 L1202.12 51.6733 L1167.85 51.6733 Q1168.34 59.3701 1172.47 63.421 Q1176.64 67.4314 1184.06 67.4314 Q1188.35 67.4314 1192.36 66.3781 Q1196.41 65.3249 1200.38 63.2184 L1200.38 70.267 Q1196.37 71.9684 1192.16 72.8596 Q1187.95 73.7508 1183.61 73.7508 Q1172.76 73.7508 1166.4 67.4314 Q1160.08 61.1119 1160.08 50.3365 Q1160.08 39.1965 1166.07 32.6746 Q1172.11 26.1121 1182.32 26.1121 Q1191.47 26.1121 1196.78 32.0264 Q1202.12 37.9003 1202.12 48.0275 M1194.67 45.84 Q1194.59 39.7232 1191.23 36.0774 Q1187.91 32.4315 1182.4 32.4315 Q1176.16 32.4315 1172.39 35.9558 Q1168.66 39.4801 1168.1 45.8805 L1194.67 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1277.31 14.0809 L1277.31 22.0612 Q1272.65 19.8332 1268.52 18.7395 Q1264.39 17.6457 1260.54 17.6457 Q1253.85 17.6457 1250.21 20.2383 Q1246.6 22.8309 1246.6 27.611 Q1246.6 31.6214 1248.99 33.6873 Q1251.42 35.7128 1258.15 36.9686 L1263.09 37.9813 Q1272.25 39.7232 1276.58 44.1387 Q1280.96 48.5136 1280.96 55.8863 Q1280.96 64.6767 1275.04 69.2137 Q1269.17 73.7508 1257.78 73.7508 Q1253.49 73.7508 1248.63 72.7785 Q1243.81 71.8063 1238.62 69.9024 L1238.62 61.4765 Q1243.61 64.2716 1248.39 65.6895 Q1253.17 67.1073 1257.78 67.1073 Q1264.79 67.1073 1268.6 64.3527 Q1272.41 61.598 1272.41 56.4939 Q1272.41 52.0379 1269.65 49.5264 Q1266.94 47.0148 1260.7 45.759 L1255.72 44.7868 Q1246.56 42.9639 1242.47 39.075 Q1238.38 35.1862 1238.38 28.2591 Q1238.38 20.2383 1244.01 15.6203 Q1249.68 11.0023 1259.61 11.0023 Q1263.86 11.0023 1268.28 11.7719 Q1272.69 12.5416 1277.31 14.0809 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1300.76 14.324 L1300.76 27.2059 L1316.12 27.2059 L1316.12 32.9987 L1300.76 32.9987 L1300.76 57.6282 Q1300.76 63.1779 1302.26 64.7578 Q1303.8 66.3376 1308.46 66.3376 L1316.12 66.3376 L1316.12 72.576 L1308.46 72.576 Q1299.83 72.576 1296.55 69.3758 Q1293.27 66.1351 1293.27 57.6282 L1293.27 32.9987 L1287.8 32.9987 L1287.8 27.2059 L1293.27 27.2059 L1293.27 14.324 L1300.76 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1364.73 48.0275 L1364.73 51.6733 L1330.46 51.6733 Q1330.94 59.3701 1335.08 63.421 Q1339.25 67.4314 1346.66 67.4314 Q1350.95 67.4314 1354.97 66.3781 Q1359.02 65.3249 1362.99 63.2184 L1362.99 70.267 Q1358.98 71.9684 1354.76 72.8596 Q1350.55 73.7508 1346.22 73.7508 Q1335.36 73.7508 1329 67.4314 Q1322.68 61.1119 1322.68 50.3365 Q1322.68 39.1965 1328.67 32.6746 Q1334.71 26.1121 1344.92 26.1121 Q1354.07 26.1121 1359.38 32.0264 Q1364.73 37.9003 1364.73 48.0275 M1357.27 45.84 Q1357.19 39.7232 1353.83 36.0774 Q1350.51 32.4315 1345 32.4315 Q1338.76 32.4315 1334.99 35.9558 Q1331.27 39.4801 1330.7 45.8805 L1357.27 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1384.17 65.7705 L1384.17 89.8329 L1376.68 89.8329 L1376.68 27.2059 L1384.17 27.2059 L1384.17 34.0924 Q1386.52 30.0415 1390.09 28.0971 Q1393.69 26.1121 1398.67 26.1121 Q1406.94 26.1121 1412.08 32.6746 Q1417.27 39.2371 1417.27 49.9314 Q1417.27 60.6258 1412.08 67.1883 Q1406.94 73.7508 1398.67 73.7508 Q1393.69 73.7508 1390.09 71.8063 Q1386.52 69.8214 1384.17 65.7705 M1409.53 49.9314 Q1409.53 41.7081 1406.13 37.0496 Q1402.77 32.3505 1396.85 32.3505 Q1390.94 32.3505 1387.53 37.0496 Q1384.17 41.7081 1384.17 49.9314 Q1384.17 58.1548 1387.53 62.8538 Q1390.94 67.5124 1396.85 67.5124 Q1402.77 67.5124 1406.13 62.8538 Q1409.53 58.1548 1409.53 49.9314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1456.32 12.096 L1491.08 12.096 L1491.08 18.9825 L1464.5 18.9825 L1464.5 36.8065 L1488.48 36.8065 L1488.48 43.6931 L1464.5 43.6931 L1464.5 72.576 L1456.32 72.576 L1456.32 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1498.41 54.671 L1498.41 27.2059 L1505.86 27.2059 L1505.86 54.3874 Q1505.86 60.8284 1508.37 64.0691 Q1510.88 67.2693 1515.91 67.2693 Q1521.94 67.2693 1525.43 63.421 Q1528.95 59.5726 1528.95 52.9291 L1528.95 27.2059 L1536.41 27.2059 L1536.41 72.576 L1528.95 72.576 L1528.95 65.6084 Q1526.24 69.7404 1522.63 71.7658 Q1519.07 73.7508 1514.33 73.7508 Q1506.51 73.7508 1502.46 68.8897 Q1498.41 64.0286 1498.41 54.671 M1517.16 26.1121 L1517.16 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1589.47 45.1919 L1589.47 72.576 L1582.02 72.576 L1582.02 45.4349 Q1582.02 38.994 1579.51 35.7938 Q1577 32.5936 1571.97 32.5936 Q1565.94 32.5936 1562.45 36.4419 Q1558.97 40.2903 1558.97 46.9338 L1558.97 72.576 L1551.47 72.576 L1551.47 27.2059 L1558.97 27.2059 L1558.97 34.2544 Q1561.64 30.163 1565.25 28.1376 Q1568.89 26.1121 1573.63 26.1121 Q1581.45 26.1121 1585.46 30.9732 Q1589.47 35.7938 1589.47 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1636.99 28.9478 L1636.99 35.9153 Q1633.83 34.1734 1630.63 33.3227 Q1627.47 32.4315 1624.23 32.4315 Q1616.98 32.4315 1612.97 37.0496 Q1608.96 41.6271 1608.96 49.9314 Q1608.96 58.2358 1612.97 62.8538 Q1616.98 67.4314 1624.23 67.4314 Q1627.47 67.4314 1630.63 66.5807 Q1633.83 65.6895 1636.99 63.9476 L1636.99 70.8341 Q1633.87 72.2924 1630.51 73.0216 Q1627.19 73.7508 1623.42 73.7508 Q1613.17 73.7508 1607.13 67.3098 Q1601.1 60.8689 1601.1 49.9314 Q1601.1 38.832 1607.17 32.472 Q1613.29 26.1121 1623.9 26.1121 Q1627.35 26.1121 1630.63 26.8413 Q1633.91 27.5299 1636.99 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1657.32 14.324 L1657.32 27.2059 L1672.68 27.2059 L1672.68 32.9987 L1657.32 32.9987 L1657.32 57.6282 Q1657.32 63.1779 1658.82 64.7578 Q1660.36 66.3376 1665.02 66.3376 L1672.68 66.3376 L1672.68 72.576 L1665.02 72.576 Q1656.39 72.576 1653.11 69.3758 Q1649.83 66.1351 1649.83 57.6282 L1649.83 32.9987 L1644.36 32.9987 L1644.36 27.2059 L1649.83 27.2059 L1649.83 14.324 L1657.32 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1682.48 27.2059 L1689.93 27.2059 L1689.93 72.576 L1682.48 72.576 L1682.48 27.2059 M1682.48 9.54393 L1689.93 9.54393 L1689.93 18.9825 L1682.48 18.9825 L1682.48 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1723.11 32.4315 Q1717.12 32.4315 1713.63 37.1306 Q1710.15 41.7891 1710.15 49.9314 Q1710.15 58.0738 1713.59 62.7728 Q1717.08 67.4314 1723.11 67.4314 Q1729.07 67.4314 1732.55 62.7323 Q1736.03 58.0333 1736.03 49.9314 Q1736.03 41.8701 1732.55 37.1711 Q1729.07 32.4315 1723.11 32.4315 M1723.11 26.1121 Q1732.83 26.1121 1738.38 32.4315 Q1743.93 38.7509 1743.93 49.9314 Q1743.93 61.0714 1738.38 67.4314 Q1732.83 73.7508 1723.11 73.7508 Q1713.35 73.7508 1707.8 67.4314 Q1702.29 61.0714 1702.29 49.9314 Q1702.29 38.7509 1707.8 32.4315 Q1713.35 26.1121 1723.11 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip750)\" d=\"M1794 45.1919 L1794 72.576 L1786.55 72.576 L1786.55 45.4349 Q1786.55 38.994 1784.04 35.7938 Q1781.53 32.5936 1776.5 32.5936 Q1770.47 32.5936 1766.98 36.4419 Q1763.5 40.2903 1763.5 46.9338 L1763.5 72.576 L1756 72.576 L1756 27.2059 L1763.5 27.2059 L1763.5 34.2544 Q1766.17 30.163 1769.78 28.1376 Q1773.42 26.1121 1778.16 26.1121 Q1785.98 26.1121 1789.99 30.9732 Q1794 35.7938 1794 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip752)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.067,1386.4 328.444,1386.4 507.612,1386.4 615.285,1386.4 706.321,1386.4 805.211,1386.4 896.379,1386.4 994.121,1386.4 1107.36,1386.4 1157.68,1386.4 \n",
       "  1208,1386.4 1230.07,1386.4 1252.13,1386.4 1263.16,1386.4 1274.19,1386.4 1279.71,1386.4 1285.22,1386.4 1287.98,1386.4 1290.74,1386.4 1293.49,1386.4 \n",
       "  1296.25,1386.4 1297.76,1386.4 1299.26,1386.4 1300.01,1386.4 1300.76,1386.4 1301.51,160.256 1302.27,160.256 1303.02,160.256 1303.77,160.256 1304.52,160.256 \n",
       "  1305.27,160.256 1306.78,160.256 1308.28,160.256 1314.29,160.256 1320.31,160.256 1332.33,160.256 1344.36,160.256 1368.41,160.256 1392.47,160.256 1440.86,160.256 \n",
       "  1489.26,160.256 1596.27,160.256 1698.09,160.256 1803.96,160.256 1893.07,160.256 2001.88,160.256 2104.16,160.256 2268.01,160.256 2293.23,160.256 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#= \n",
    "Heaviside Step Function\n",
    "\n",
    "    Parameters:\n",
    "        Some integer 'z'.\n",
    "\n",
    "    Returns:\n",
    "        0 for negative argument 'z'.\n",
    "        1 for positive argument 'z'.\n",
    "=#\n",
    "\n",
    "function H(z)\n",
    "    \n",
    "        if z < 0\n",
    "            return 0\n",
    "        \n",
    "        else\n",
    "            return 1\n",
    "        \n",
    "        end\n",
    "end\n",
    "\n",
    "#= \n",
    "Plot Heaviside Step Function \n",
    "=#\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot(z -> H(z), title = \"Heaviside Step Function\", lw = 3, ylabel=\"H(z)\", xlabel=\"z\", label=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f57231",
   "metadata": {},
   "source": [
    "# The Perceptron Training Rule <a class=\"anchor\" id=\"tptr\"></a>\n",
    "\n",
    "The Perceptron Training Rule is a method for iteratively modifying a perceptron's weights whenever it returns an undesireable output. \n",
    "\n",
    "Consider the weight value $w_{i}$ from the perceptron weight vector $W$, The Perceptron Training Rule States:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hspace{0.3cm} \\Large w_{i} =  w_{i} + Δw_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hspace{0.3cm} \\Large Δw_{i} = \\alpha*(y-ŷ)*x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "In other words:\n",
    "\\begin{equation}\n",
    "\\hspace{0.3cm} \\large Updated \\hspace{0.2cm} Weight \\hspace{0.2cm} Value =  Old \\hspace{0.2cm}  Weight \\hspace{0.2cm} Value + Weight \\hspace{0.2cm} Update\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large Weight \\hspace{0.2cm} Update =  Learning \\hspace{0.2cm}  Rate * Error * Input \\hspace{0.2cm} Value\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b902a4c3",
   "metadata": {},
   "source": [
    "#### Error Function <a class=\"anchor\" id=\"error\"></a>\n",
    "\n",
    "The perceptron error measures the distance of the perceptron output (*ŷ*) from the training/expected output (*y*).\n",
    "\n",
    "The error value plays a role in how far the perceptron weights will be 'nudged' during training.\n",
    "\n",
    "\\begin{equation}\n",
    "\\\\\n",
    "\\hspace{0.3cm} \\large Output \\hspace{0.2cm} Error =  Expected \\hspace{0.2cm} Output - Perceptron \\hspace{0.2cm} Output\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86781b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output_error (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#= \n",
    "Output Error\n",
    "    \n",
    "    Parameters:\n",
    "        Some label/expected output 'y'.\n",
    "        Some Perceptron 'p'.\n",
    "\n",
    "    Returns:\n",
    "        The difference between the expected output 'y' and the calculated perceptron output 'ŷ'.\n",
    "=#\n",
    "\n",
    "function output_error(y, p::Perceptron)\n",
    "    \n",
    "    return y - p.ŷ\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e38a6",
   "metadata": {},
   "source": [
    "#### Delta W <a class=\"anchor\" id=\"deltaw\"></a>\n",
    "\n",
    "$Δw_{i}$ represents how much the weights vector $W$ should change in order to update it.\n",
    "\n",
    "$α$ is the learning rate, if it's too large weights will never converge and if it is too small they will take a very long time to converge. The learning rate moderates the weight changes so they do not go too far in any given direction.\n",
    "\n",
    "$Δw_{i}$ also takes in the error function (as stated above) and the input vector for a given step.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hspace{0.3cm} \\Large Δw_{i} = \\alpha*(y-ŷ)*x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\large Weight \\hspace{0.2cm} Update =  Learning \\hspace{0.2cm}  Rate * Error * Input \\hspace{0.2cm} Value\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a3f579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delta_w (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#= \n",
    "Delta W\n",
    "    \n",
    "    Parameters:\n",
    "        Some learning rate 'α'.\n",
    "        Some error value 'error'.\n",
    "        Some perceptron 'p'.\n",
    "\n",
    "    Returns:\n",
    "        A vector (ΔW) suggesting the degree in which the weights vector of the perceptron (weights) should be changed.\n",
    "\n",
    "        Where ΔW = learning Rate(α) X Output Error(error) X Input Vector(inputs).\n",
    "=#\n",
    "\n",
    "function delta_w( α, error, p::Perceptron)\n",
    "    \n",
    "    return α * error * p.X\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8aeb2e",
   "metadata": {},
   "source": [
    "#### Update Weights <a class=\"anchor\" id=\"uweights\"></a>\n",
    "\n",
    "This function updates each $w_{i}$ value from the weight vector $W$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hspace{0.3cm} \\Large w_{i} =  w_{i} + Δw_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hspace{0.3cm} \\large Updated \\hspace{0.2cm} Weight \\hspace{0.2cm} Value =  Old \\hspace{0.2cm}  Weight \\hspace{0.2cm} Value + Weight \\hspace{0.2cm} Update\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737cf5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_weights (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#= \n",
    "Update Weights\n",
    "    \n",
    "    Parameters:\n",
    "        Some vector 'ΔW'.\n",
    "        Some Perceptron 'p'.\n",
    "\n",
    "    Returns:\n",
    "        The updated weight vector (weights) of the Perceptron. \n",
    "        \n",
    "        Where weights(updated) =  weights(old) +  ΔW.\n",
    "=#\n",
    "\n",
    "function update_weights(ΔW, p::Perceptron)\n",
    "    \n",
    "    p.W = p.W + ΔW\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f9a8e",
   "metadata": {},
   "source": [
    "#### Backward Pass <a class=\"anchor\" id=\"bpass\"></a>\n",
    "\n",
    "This function applies the Perceptron Training Rule in concert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247a14cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_pass (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#= \n",
    "Backward Pass back through the Perceptron\n",
    "    \n",
    "    1. Calculate the output error of the Perceptron.\n",
    "    2. Calculate the necesarry weight change (ΔW).\n",
    "    3. Update the Perceptron weights (weights) according to the weight change.\n",
    "\n",
    "    Parameters:\n",
    "        Some Perceptron 'p'.\n",
    "        Some label/expected output 'y'.\n",
    "        Some learning rate 'α'.\n",
    "\n",
    "    Returns:\n",
    "        The updated weight vector (W1) of the Perceptron. \n",
    "=#\n",
    "\n",
    "function back_pass(p::Perceptron, y, α)\n",
    "    \n",
    "    error = output_error(y, p)\n",
    "    \n",
    "    ΔW = delta_w(α, error, p)\n",
    "    \n",
    "    update_weights(ΔW, p)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9989da6",
   "metadata": {},
   "source": [
    "# Training <a class=\"anchor\" id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45452b57",
   "metadata": {},
   "source": [
    "#### Import Training Data <a class=\"anchor\" id=\"imdata\"></a>\n",
    "\n",
    "The training data for a logic gate perceptron is essentially a truth table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00f7c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 4 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>x</th><th>y</th><th>and</th><th>or</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th></tr></thead><tbody><tr><th>1</th><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>2</th><td>0</td><td>1</td><td>0</td><td>1</td></tr><tr><th>3</th><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><th>4</th><td>1</td><td>1</td><td>1</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& x & y & and & or\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0 & 0 & 0 & 0 \\\\\n",
       "\t2 & 0 & 1 & 0 & 1 \\\\\n",
       "\t3 & 1 & 0 & 0 & 1 \\\\\n",
       "\t4 & 1 & 1 & 1 & 1 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m x     \u001b[0m\u001b[1m y     \u001b[0m\u001b[1m and   \u001b[0m\u001b[1m or    \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\n",
       "─────┼────────────────────────────\n",
       "   1 │     0      0      0      0\n",
       "   2 │     0      1      0      1\n",
       "   3 │     1      0      0      1\n",
       "   4 │     1      1      1      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#= \n",
    "Import Training Dataset\n",
    "=#\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "train_set = DataFrame(CSV.File(\"trainset.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d1485",
   "metadata": {},
   "source": [
    "#### Training AND-perceptron <a class=\"anchor\" id=\"tandp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aeb2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= \n",
    "Training AND-perceptron\n",
    "=#\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "p_and = Perceptron(2)\n",
    "\n",
    "while epoch < 100\n",
    "    \n",
    "    for i in 1:nrow(train_set)\n",
    "        \n",
    "        p_and.X = Vector(train_set[i, 1:2])\n",
    "        \n",
    "        forward_pass(p_and)\n",
    "        \n",
    "        back_pass(p_and, train_set[i, 3], 0.1)\n",
    "        \n",
    "        \n",
    "    end\n",
    "    \n",
    "    global epoch += 1\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f502d1",
   "metadata": {},
   "source": [
    "#### Training OR-perceptron <a class=\"anchor\" id=\"torp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ec2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= \n",
    "Training OR-Perceptron\n",
    "=#\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "p_or = Perceptron(2)\n",
    "\n",
    "while epoch < 100\n",
    "    \n",
    "    \n",
    "    for i in 1:nrow(train_set)\n",
    "        \n",
    "        p_or.X = Vector(train_set[i, 1:2])\n",
    "        \n",
    "        forward_pass(p_or)\n",
    "        \n",
    "        back_pass(p_or, train_set[i, 4], 0.1)\n",
    "        \n",
    "    end\n",
    "    \n",
    "    global epoch += 1\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8083c4",
   "metadata": {},
   "source": [
    "# Results <a class=\"anchor\" id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eacc0fc",
   "metadata": {},
   "source": [
    "#### Output Function <a class=\"anchor\" id=\"output\"></a>\n",
    "\n",
    "A function to pull an output from some perceptron given some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7efb85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#= \n",
    "Output Function\n",
    "    \n",
    "    Parameters:\n",
    "        Some Perceptron 'p'.\n",
    "        Some 2-element input vector 'input'.\n",
    "\n",
    "    Returns:\n",
    "        The calculated Perceptron output 'ŷ'.\n",
    "=#\n",
    "\n",
    "function output(p::Perceptron, input::Vector{Int64})\n",
    "    \n",
    "    p.X = input\n",
    "    \n",
    "    return forward_pass(p)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c52a3",
   "metadata": {},
   "source": [
    "#### AND-perceptron Results <a class=\"anchor\" id=\"apr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d394e4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(p_and, [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b1f89d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(p_and, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd279cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(p_and, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3eab5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(p_and, [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096a920",
   "metadata": {},
   "source": [
    "#### OR-perceptron Results <a class=\"anchor\" id=\"opr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4774d5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(p_or, [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66c1f0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(p_or, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51cf1fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(p_or, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b6a1ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(p_or, [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d13f3",
   "metadata": {},
   "source": [
    "# Conclusion <a class=\"anchor\" id=\"conc\"></a>\n",
    "\n",
    "The perceptron is a supervised machine learning algorithm that is able to operate as a boolean function on linearly separable data.\n",
    "\n",
    "This project demonstrates the building, training and use of a perceptron as an AND/OR logic gate.\n",
    "\n",
    "Rosenblatt's work paved the way for the huge amount of research and real world applications we see in the field of deep learning today. Perceptrons are especicially useful when they are connected together within a neural network.\n",
    "\n",
    "Backpropogation is a machine learning algorithm that allows feed-forward neural networks to learn. Building and training a neural network is a natural follow up project to the perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b01a690",
   "metadata": {},
   "source": [
    "# References <a class=\"anchor\" id=\"ref\"></a>\n",
    "\n",
    "1. https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon\n",
    "2. https://www.mldawn.com/the-perceptron-training-rule/\n",
    "3. https://electronicsclub.info/gates.htm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
